{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary\n\nCommon notebook for reusable components (classes, functions, etc.) that can be reused in other notebooks.","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport cv2\nimport math\nimport copy\nimport time\nimport random\nimport glob\nfrom matplotlib import pyplot as plt\n\n# For data manipulation\nimport numpy as np\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchmetrics\n\n# For Image Models\nimport timm\n\n# Albumentations for augmentations\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# Utils\nimport joblib\nfrom tqdm import tqdm\nfrom collections import defaultdict\n\n# For colored terminal text\nfrom colorama import Fore, Back, Style\nb_ = Fore.BLUE\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# For descriptive error messages\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-03T13:30:13.364896Z","iopub.execute_input":"2024-01-03T13:30:13.365301Z","iopub.status.idle":"2024-01-03T13:30:26.193339Z","shell.execute_reply.started":"2024-01-03T13:30:13.365268Z","shell.execute_reply":"2024-01-03T13:30:26.192084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CONFIG = {\n    \"seed\": 42,\n    \"epochs\": 16,\n    \"img_size\": 96,\n    \"crop_size\": 32,\n    \"model_name\": \"tf_efficientnet_b0\",\n    \"weights_name\": None, # None means no pretrained weights\n    \"num_classes\": 1,\n    \"train_batch_size\": 32,\n    \"valid_batch_size\": 32,\n    \"tta_size\": 4,\n    \"learning_rate\": 1e-4,\n    \n    \"scheduler\": \"CyclicLR\",\n    \"cl_base_lr\": 1e-6,\n    \"cl_max_lr\": 1e-4,\n    \"cl_step_size\": None, # for both up and down, to be calculated\n    \n    #\"scheduler\": 'CosineAnnealingLR',\n    #\"min_lr\": 1e-6,\n    #\"T_max\": 500,\n    \n    \"weight_decay\": 1e-6,\n    \"train_size\": 8000, # To have less training times, we do not want to train all for the baseline\n    \"val_size\" : 200,\n    \"n_accumulate\": 1,\n    \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n}\n\n\nIS_INTERACTIVE = ('runtime' in get_ipython().config.IPKernelApp.connection_file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed=42):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    # When running on the CuDNN backend, two further options must be set\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    # Set a fixed value for the hash seed\n    os.environ['PYTHONHASHSEED'] = str(seed)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset class","metadata":{}},{"cell_type":"code","source":"class HCDDataset(Dataset):\n    def __init__(self, df, transforms=None, transformed_images_per_item=1):\n        \"\"\"\n        transformed_images_per_item: how many transformed images we want to return in each item\n            In training it should always rreturn 1\n            In validation, depends on how many we want to use in TTA\n        \"\"\"\n        self.file_names = df['file_path'].values\n        self.labels = df['label'].values\n        self.transforms = transforms\n        self.transformed_images_per_item = transformed_images_per_item\n        \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, index):\n        img_path = self.file_names[index]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        label = self.labels[index]\n        \n        imgs = []\n        if self.transforms:\n            for i in range(self.transformed_images_per_item):\n                imgs.append(self.transforms(image=img)[\"image\"])\n        else:\n            imgs = [img]\n\n        return {\n            'image': torch.stack(imgs, dim=0),\n            'label': torch.tensor(label, dtype=torch.float)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Augmentations","metadata":{}},{"cell_type":"code","source":"data_transforms = {\n    \"train\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Flip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=1.0),\n        A.ShiftScaleRotate(shift_limit=0.1, \n                           scale_limit=0.15, \n                           rotate_limit=60, \n                           p=0.5),\n        A.Transpose(),\n        #A.RGBShift(),\n        \n        A.HueSaturationValue(\n                hue_shift_limit=0.2, \n                sat_shift_limit=0.2, \n                val_shift_limit=0.2, \n                p=0.5\n            ),\n        A.RandomBrightnessContrast(\n                brightness_limit=(-0.1,0.1), \n                contrast_limit=(-0.1, 0.1), \n                p=0.5\n            ),\n        A.RandomGamma(p=0.5),\n        A.CLAHE(p=0.5),\n        A.ChannelShuffle(p=0.5),\n        #A.Blur(),\n        A.GaussNoise(),\n        #A.ElasticTransform(),\n        A.Normalize(p=1),\n        ToTensorV2()], p=1.),\n    \n    \"valid\": A.Compose([\n        A.Resize(CONFIG['img_size'], CONFIG['img_size']),\n        A.Flip(p=0.5),\n        A.VerticalFlip(p=0.5),\n        A.RandomRotate90(p=1.0),\n        A.Transpose(),\n        A.Normalize(p=1),\n        ToTensorV2()], p=1.)\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"def load_weights(model, weights_path, device=CONFIG[\"device\"]):\n    if os.path.isfile(weights_path):\n        model.load_state_dict(torch.load(weights_path, map_location=device))\n        print(\"Loaded weights from\", weights_path)\n        return True\n    else:\n        print(\"No previous weights available at\", weights_path)\n        return False","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AdaptiveConcatPool2d(nn.Module):\n    def __init__(self, pool1, pool2):\n        super(AdaptiveConcatPool2d, self).__init__()\n        \n        self.pool1 = pool1\n        self.pool2 = pool2\n    \n    def forward(self, x):\n        x1 = self.pool1(x)\n        x2 = self.pool2(x)\n        return torch.cat((x1, x2), dim=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class HCDModel_Res50(nn.Module):\n    def __init__(self, num_classes=1, weights=None, dropout_p=0.5):\n        super(HCDModel_Res50, self).__init__()\n        \n        self.cnn = torchvision.models.resnet50(weights=weights)\n        in_features = self.cnn.fc.in_features * 2 # *2 because of contact pooling layers\n        self.cnn.fc = nn.Identity() # we will add dense layers manually out side of resnet\n        self.cnn.avgpool = AdaptiveConcatPool2d(nn.AdaptiveAvgPool2d((1, 1)), nn.AdaptiveMaxPool2d((1, 1)))\n        \n        print(\"in_features\", in_features)\n        self.bn1 = nn.BatchNorm1d(in_features)\n        self.linear1 = nn.Linear(in_features, 1024)\n        \n        self.bn2 = nn.BatchNorm1d(1024)\n        self.linear2 = nn.Linear(1024, 512)\n        \n        self.bn3 = nn.BatchNorm1d(512)\n        self.linear3 = nn.Linear(512, num_classes)\n          \n        self.model = nn.Sequential(\n            self.cnn,\n            self.bn1,\n            self.linear1,\n            nn.ReLU(),\n            nn.Dropout(dropout_p),\n            self.bn2,\n            self.linear2,\n            nn.ReLU(),\n            nn.Dropout(dropout_p),\n            self.bn3,\n            self.linear3,\n            nn.Sigmoid(),\n        )\n    \n    def forward(self, images):\n        return self.model(images)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss function","metadata":{}},{"cell_type":"markdown","source":"## Training and validation functions","metadata":{}},{"cell_type":"code","source":"def train_one_epoch(model, optimizer, scheduler, train_dataloader, val_dataloader, device, epoch, val_iterations=None, disable_progress_bar=False):\n    \n    \n    dataset_size = 0\n    running_loss = 0.0\n    \n    b_acc = torchmetrics.classification.BinaryAccuracy().to(device)\n    b_auroc = torchmetrics.classification.BinaryAUROC().to(device)\n    \n    bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), disable=disable_progress_bar)\n    \n    total_step = len(train_dataloader)\n    log_step = math.ceil(total_step / 10) # we log each epoch 10 times in log\n\n    epoch_start_time = time.time()\n    \n    # If not set, do 1 validation per epoch\n    if val_iterations is None:\n        val_iterations = len(train_dataloader)\n        \n    history = defaultdict(list)\n    best_valid_auroc = -np.inf\n    best_valid_loss = np.inf\n    best_valid_model_wts = None\n    \n    for step, data in bar:\n        model.train()\n        optimizer.zero_grad()\n        \n        images = data['image'].to(device, dtype=torch.float)\n        labels = data['label'].to(device, dtype=torch.float)\n        \n        # Need to squeeze the 1 dim due to support of TTA in val\n        images = torch.squeeze(images)\n        batch_size = images.size(0)\n        \n        outputs = torch.squeeze(model(images))\n        \n        loss = torch.nn.BCELoss()(outputs, labels)\n        \n        loss.backward()\n        optimizer.step()\n        \n        if not CONFIG[\"scheduler\"] == \"ReduceLROnPlateau\":\n            scheduler.step()\n            \n        #_, predicted = torch.max(outputs, 1)\n        predicted = outputs\n        \n        b_acc(predicted, labels)\n        train_acc = b_acc.compute().item()\n        \n        b_auroc(predicted, labels)\n        train_auroc = b_auroc.compute().item()\n        \n        running_loss += (loss.item() * batch_size)\n\n        dataset_size += batch_size\n        \n        train_loss = running_loss / dataset_size\n        \n        epoch_time = round(time.time() - epoch_start_time)\n        bar.set_postfix(Epoch=epoch, Epoch_Time=epoch_time, Train_Loss=train_loss, Train_Acc=train_acc,Train_AUROC=train_auroc,\n                        LR=f\"{optimizer.param_groups[0]['lr']}~{optimizer.param_groups[-1]['lr']}\")\n        \n        if disable_progress_bar and (step%log_step==0):\n            print(step, \"/\", total_step)\n        \n\n    \n        if (step+1) % val_iterations == 0:\n            val_loss, val_acc, val_auroc = valid(\n                model, val_dataloader, device=CONFIG['device'],\n                disable_progress_bar=(not IS_INTERACTIVE)\n            )\n             \n            history['Train Loss'].append(train_loss)\n            history['Valid Loss'].append(val_loss)\n            history['Train Accuracy'].append(train_acc)\n            history['Valid Accuracy'].append(val_acc)\n            history['Train AUROC'].append(train_auroc)\n            history['Valid AUROC'].append(val_auroc)\n            #history['lr'].append(scheduler.get_lr()[0])\n            \n            if  val_loss < best_valid_loss:\n                best_valid_auroc = val_auroc\n                best_valid_loss = val_loss\n                best_valid_model_wts = copy.deepcopy(model.state_dict())\n    \n    if CONFIG[\"scheduler\"] == \"ReduceLROnPlateau\":\n        scheduler.step(best_valid_loss)\n\n    if disable_progress_bar:\n        print(f\"Epoch={epoch}, Epoch_Time={epoch_time}, Train_Loss={train_loss}, Train_Acc={train_acc}, Train_AUROC={train_auroc},LR={optimizer.param_groups[0]['lr']}~{optimizer.param_groups[-1]['lr']}\")\n\n    \n    gc.collect()\n    \n    return best_valid_auroc, best_valid_loss, best_valid_model_wts, history\n\n@torch.inference_mode()\ndef valid(model, dataloader, device, disable_progress_bar=False):\n    model.eval()\n    \n    dataset_size = 0\n    running_loss = 0.0\n    b_acc = torchmetrics.classification.BinaryAccuracy().to(device)\n    b_auroc = torchmetrics.classification.BinaryAUROC().to(device)\n    \n    for step, data in enumerate(dataloader): \n        images = data['image'].to(device, dtype=torch.float)\n        labels = data['label'].to(device, dtype=torch.float)\n        \n        batch_size = images.size(0)\n        \n        # Process TTA by splitting the input image tensor\n        # split along the dim=1, which is multiple transformed images for TTA\n        images = torch.chunk(images, chunks=images.size(1), dim=1)\n        \n        outputs = []\n        \n        for image in images:\n            outputs.append(torch.squeeze(model(torch.squeeze(image))))\n        outputs = torch.sum(torch.stack(outputs), dim=0) / len(images)\n        \n\n        loss = torch.nn.BCELoss()(outputs, labels)\n\n        #_, predicted = torch.max(outputs, 1)\n        predicted = outputs\n        acc = torch.sum( predicted == labels )\n        \n        b_acc(predicted, labels)\n        epoch_acc = b_acc.compute().item()\n        \n        b_auroc(predicted, labels)\n        epoch_auroc = b_auroc.compute().item()\n        \n        running_loss += (loss.item() * batch_size)        \n        dataset_size += batch_size\n        epoch_loss = running_loss / dataset_size\n        \n    print(f\"Valid_Loss={epoch_loss}, Valid_Acc={epoch_acc}, Valid_AUROC={epoch_auroc}\")\n        \n    \n    gc.collect()\n    \n    return epoch_loss, epoch_acc, epoch_auroc\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run training function","metadata":{}},{"cell_type":"code","source":"def run_training(model, optimizer, scheduler, device, num_epochs, train_dataloader, val_dataloader, k_fold,\n                 val_iterations=None, \n                 stop_epochs=4, # If val_loss stops improve for such epochs, stop the training\n                 save_all_epoch=False, # If models for each epoch regardless of performance should be saved\n                ):\n    print(\"Run training for folder\", k_fold)\n    \n    start = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_epoch_loss = np.inf\n    best_epoch_auroc = -np.inf\n    history = defaultdict(list)\n    \n    stop = 0\n    for epoch in range(1, num_epochs + 1): \n        gc.collect()\n        valid_auroc, valid_loss, valid_model_wts, epoch_history = train_one_epoch(\n            model, optimizer, scheduler, \n            train_dataloader=train_dataloader, \n            val_dataloader=val_dataloader,\n            device=CONFIG['device'], epoch=epoch,\n            disable_progress_bar=(not IS_INTERACTIVE),\n            val_iterations=val_iterations,\n        )\n    \n        history['Train Loss'].extend(epoch_history['Train Loss'])\n        history['Valid Loss'].extend(epoch_history['Valid Loss'])\n        history['Train Accuracy'].extend(epoch_history['Train Accuracy'])\n        history['Valid Accuracy'].extend(epoch_history['Valid Accuracy'])\n        history['Train AUROC'].extend(epoch_history['Train AUROC'])\n        history['Valid AUROC'].extend(epoch_history['Valid AUROC'])\n        #history['lr'].extend(epoch_history['lr'])\n        \n        # deep copy the model\n        improved = False\n        if valid_loss < best_epoch_loss:\n            improved = True\n            print(f\"{b_}Validation loss improved ({best_epoch_loss} ---> {valid_loss})\")\n            best_epoch_loss = valid_loss\n            best_epoch_auroc = valid_auroc\n            stop = 0 # loss improved, reset stop\n        else:\n            stop +=1\n            \n        if improved or save_all_epoch:\n            PATH = \"AUROC{:.2f}_Loss{:.4f}_fold{:.0f}_epoch{:.0f}.bin\".format(valid_auroc, valid_loss, k_fold, epoch)\n            torch.save(valid_model_wts, PATH)\n            # Save a model file from the current directory\n            print(f\"Model Saved{sr_}\")\n            \n        if stop >= stop_epochs:\n            print(f\"Performance havn't improve for {stop} epochs, stop training!\")\n            break\n    \n    end = time.time()\n    time_elapsed = end - start\n    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n    print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n    print(\"-----------------------------------------------------\")\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    \n    return model, history, best_epoch_loss, best_epoch_auroc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_optim_params(model):\n    \"\"\"\n    Config different lr for different layers, the deeper the smaller\n    \"\"\" \n    return [\n        {'params': model.cnn.conv1.parameters(), 'lr': CONFIG[\"learning_rate\"]/1000},\n        {'params': model.cnn.bn1.parameters(), 'lr': CONFIG[\"learning_rate\"]/1000},\n        {'params': model.cnn.layer1.parameters(), 'lr': CONFIG[\"learning_rate\"]/100},\n        {'params': model.cnn.layer2.parameters(), 'lr': CONFIG[\"learning_rate\"]/100},\n        {'params': model.cnn.layer3.parameters(), 'lr': CONFIG[\"learning_rate\"]/10},\n        {'params': model.cnn.layer4.parameters(), 'lr': CONFIG[\"learning_rate\"]/10},\n        {'params': model.bn1.parameters(), 'lr': CONFIG[\"learning_rate\"]/100},\n        {'params': model.bn2.parameters(), 'lr': CONFIG[\"learning_rate\"]/100},\n        {'params': model.bn3.parameters(), 'lr': CONFIG[\"learning_rate\"]/100},\n        {'params': model.linear1.parameters(), 'lr': CONFIG[\"learning_rate\"]},\n        {'params': model.linear2.parameters(), 'lr': CONFIG[\"learning_rate\"]},\n        {'params': model.linear3.parameters(), 'lr': CONFIG[\"learning_rate\"]},\n    ]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_scheduler_optimizer(model_parameters):\n    if CONFIG['scheduler'] == 'CosineAnnealingLR':\n        optimizer = optim.Adam(model_parameters, lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], eta_min=CONFIG['min_lr'])\n        \n    elif CONFIG['scheduler'] == 'CosineAnnealingWarmRestarts':\n        optimizer = optim.Adam(model_parameters, lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])\n        scheduler = lr_scheduler.CosineAnnealingWarmRestarts(optimizer,T_0=CONFIG['T_0'], \n                                                             eta_min=CONFIG['min_lr'])\n    elif CONFIG['scheduler'] == 'CyclicLR':\n        optimizer = optim.SGD(model_parameters, lr=CONFIG['learning_rate'], momentum=0.9)\n        scheduler = lr_scheduler.CyclicLR(optimizer,\n                                          base_lr=CONFIG['cl_base_lr'], \n                                          max_lr=CONFIG['cl_max_lr'],\n                                          step_size_up=CONFIG['cl_step_size'],\n                                         )\n    elif CONFIG['scheduler'] == 'ReduceLROnPlateau':\n        optimizer = optim.SGD(model_parameters, lr=CONFIG['learning_rate'], momentum=0.9)\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, min_lr=CONFIG[\"min_lr\"],\n                                                   patience=CONFIG['rlrop_patience'], threshold=CONFIG['rlrop_thr'], verbose=True)\n    elif CONFIG['scheduler'] == None:\n        return None, None\n        \n    return scheduler, optimizer","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare loaders","metadata":{}},{"cell_type":"code","source":"def prepare_loaders(train_df, val_df, tta_size=1):\n    \n    train_dataset = HCDDataset(train_df, transforms=data_transforms[\"train\"])\n    valid_dataset = HCDDataset(val_df, transforms=data_transforms[\"valid\"], transformed_images_per_item=tta_size)\n\n    train_loader = DataLoader(train_dataset, batch_size=CONFIG['train_batch_size'], \n                              num_workers=2, shuffle=True, pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=CONFIG['valid_batch_size'], \n                              num_workers=2, shuffle=False, pin_memory=True)\n    \n    return train_loader, valid_loader","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot history","metadata":{}},{"cell_type":"code","source":"def plot_history(history, metric_name):\n    plt.plot( range(history.shape[0]), history[f\"Train {metric_name}\"].values, label=f\"Train {metric_name}\")\n    plt.plot( range(history.shape[0]), history[f\"Valid {metric_name}\"].values, label=f\"Valid {metric_name}\")\n    plt.xlabel(\"Probs\")\n    plt.ylabel(metric_name)\n    plt.grid()\n    plt.legend()\n    plt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Generate submit csv","metadata":{}},{"cell_type":"code","source":"def generate_submit_csv(models, root_folder, transforms):\n    test_df = pd.read_csv(os.path.join(root_folder, \"sample_submission.csv\"))\n    test_df[\"file_path\"] = test_df[\"id\"].apply(lambda image_id: os.path.join(root_folder, \"test\", f\"{image_id}.tif\"))\n    print(\"test_df shape:\", test_df.shape)\n\n    test_dataset = HCDDataset(test_df, transforms=transforms[\"valid\"], transformed_images_per_item=CONFIG[\"tta_size\"])\n    test_loader = DataLoader(test_dataset, batch_size=CONFIG['valid_batch_size'], \n                                  num_workers=2, shuffle=False, pin_memory=True)\n\n    preds = []\n\n    total_step = len(test_loader)\n    log_step = math.ceil(total_step / 10) # we log each epoch 10 times in log\n\n    with torch.no_grad():\n        bar = tqdm(enumerate(test_loader), total=len(test_loader), disable=(not IS_INTERACTIVE))\n        for step, data in bar:        \n            images = data['image'].to(CONFIG[\"device\"], dtype=torch.float)        \n            \n            outputs = []\n            for model in models:\n                \n                # Process TTA by splitting the input image tensor\n                # split along the dim=1, which is multiple transformed images for TTA\n                images = torch.chunk(images, chunks=images.size(1), dim=1)\n\n                for image in images:\n                    outputs.append(model.sigmoid(model(torch.squeeze(image))))\n \n            _, predicted = torch.max(torch.sum(torch.stack(outputs), dim=0), dim=1)\n            \n            preds.extend(predicted.tolist())\n\n            if not IS_INTERACTIVE and (step%log_step==0):\n                print(step, \"/\", total_step)\n\n    print(len(preds))\n    \n    test_df[\"label\"] = preds\n    test_df[[\"id\", \"label\"]].to_csv(\"submission.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]}]}