{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"},{"sourceId":7338335,"sourceType":"datasetVersion","datasetId":4260392},{"sourceId":7365217,"sourceType":"datasetVersion","datasetId":4278727},{"sourceId":158265341,"sourceType":"kernelVersion"}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary\n\nImproved model.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Import libs","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport random\n\n# For data manipulation\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\n\n# Pytorch Imports\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Utils\nfrom tqdm import tqdm\n\nfrom sklearn.model_selection import GroupKFold","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:47:27.703803Z","iopub.execute_input":"2024-01-09T06:47:27.704241Z","iopub.status.idle":"2024-01-09T06:47:33.023823Z","shell.execute_reply.started":"2024-01-09T06:47:27.704207Z","shell.execute_reply":"2024-01-09T06:47:33.022836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from hcd_3_common import (\n    fetch_scheduler_optimizer,\n    generate_submit_csv,\n    get_optim_params,\n    load_weights,\n    prepare_loaders,\n    plot_history,\n    run_training,\n    set_seed,\n    HCDDataset,\n    data_transforms,\n    HCDModel_Res50,\n    CONFIG,\n    IS_INTERACTIVE,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:47:33.026195Z","iopub.execute_input":"2024-01-09T06:47:33.027214Z","iopub.status.idle":"2024-01-09T06:47:39.905892Z","shell.execute_reply.started":"2024-01-09T06:47:33.027163Z","shell.execute_reply":"2024-01-09T06:47:39.904807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"ROOT_FOLDER = \"/kaggle/input/histopathologic-cancer-detection\"\n# From https://github.com/azkalot1/Histopathologic-Cancer-Detection/blob/master/patch_id_wsi.csv\n# It would fix a leak that patches from the sam WSI image are mixed in the training set, i.e.\n# Validation set would be similar to training set. We use this selecting validation set without leak.\nPATCH_ID_WSI_PATH = \"/kaggle/input/hcd-patch-id-wsi/patch_id_wsi.csv\"\n\nPSUDO_CSV_PATH = \"/kaggle/input/hcd-3-inference-result/submission.csv\"\nPSUDO_FOLDER = \"/kaggle/input/histopathologic-cancer-detection/test\"\nPSUDO_THR = 0.999\n\nPREVIOUS_BEST_WEIGHT = \"/kaggle/input/hcd-3-improve-model/AUROC0.92_Loss0.1925_epoch8.bin\"","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:47:39.907171Z","iopub.execute_input":"2024-01-09T06:47:39.907722Z","iopub.status.idle":"2024-01-09T06:47:39.916090Z","shell.execute_reply.started":"2024-01-09T06:47:39.907689Z","shell.execute_reply":"2024-01-09T06:47:39.914863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Following configurations are often overwritten\nCONFIG[\"weights_name\"] = \"IMAGENET1K_V1\" # None means no pretrained weights\nCONFIG[\"dropout_p\"] = 0.8\nCONFIG[\"val_iterations\"] = 50 # Do validation per iterations\nCONFIG[\"epochs\"] = 32\nCONFIG[\"stop_epochs\"] = 4 # Stop training when loss not improved for such epochs\nCONFIG[\"train_batch_size\"] = 256\nCONFIG[\"valid_batch_size\"] = 256\nCONFIG[\"train_size\"] = 153600\nCONFIG[\"val_size\"] = 1280\nCONFIG[\"psudo_train_size\"] = 0#None # None is all\nCONFIG[\"tta_size\"] = 4\nCONFIG[\"learning_rate\"] = 1e-4\nCONFIG[\"min_lr\"] = 1e-6\nCONFIG[\"img_size\"] = 96\nCONFIG[\"k_fold\"] = 5\n\n\nif CONFIG[\"val_iterations\"] is None:\n    # By default, 1 validation per epoch\n    CONFIG[\"val_iterations\"] = CONFIG[\"train_size\"] // CONFIG[\"train_batch_size\"]\n\n#CONFIG[\"scheduler\"] = \"ReduceLROnPlateau\"\n#ReduceLROnPlateau patience 2 epoch\n#CONFIG[\"rlrop_patience\"] = 1\n#CONFIG['rlrop_thr'] = 1e-3\n\nCONFIG[\"scheduler\"] = \"CosineAnnealingLR\"\n# Calculate the T_max, as epochs * iter per epoch\nCONFIG['T_max'] = CONFIG[\"train_size\"] * CONFIG['epochs'] // CONFIG['train_batch_size']\n\n#CONFIG[\"scheduler\"] = \"CyclicLR\"\n#CONFIG[\"cl_base_lr\"] = 1e-6\n#CONFIG[\"cl_max_lr\"] = 1e-4\n# Calculate the step size in case of cyclic LR, loop in every 2 epoch (1 epoch up, 1 epoch down)\n#CONFIG[\"cl_step_size\"] = int(CONFIG[\"train_size\"] / CONFIG[\"train_batch_size\"])\n\nprint(\"CONFIG:\", CONFIG)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:47:39.918898Z","iopub.execute_input":"2024-01-09T06:47:39.919290Z","iopub.status.idle":"2024-01-09T06:47:39.930735Z","shell.execute_reply.started":"2024-01-09T06:47:39.919256Z","shell.execute_reply":"2024-01-09T06:47:39.929300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:47:39.932101Z","iopub.execute_input":"2024-01-09T06:47:39.932623Z","iopub.status.idle":"2024-01-09T06:47:39.954294Z","shell.execute_reply.started":"2024-01-09T06:47:39.932580Z","shell.execute_reply":"2024-01-09T06:47:39.953061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"\ndf = pd.read_csv(os.path.join(ROOT_FOLDER, \"train_labels.csv\"))\ndf[\"file_path\"] = df[\"id\"].apply(lambda image_id: os.path.join(ROOT_FOLDER, \"train\", f\"{image_id}.tif\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:47:39.956176Z","iopub.execute_input":"2024-01-09T06:47:39.956834Z","iopub.status.idle":"2024-01-09T06:47:41.192833Z","shell.execute_reply.started":"2024-01-09T06:47:39.956790Z","shell.execute_reply":"2024-01-09T06:47:41.191972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"patch_id_wsi_df = pd.read_csv(PATCH_ID_WSI_PATH)\nprint(len(patch_id_wsi_df[\"wsi\"].unique()))\ndf = pd.merge(df, patch_id_wsi_df, on='id')\ndf = df.sample(frac=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:47:41.194102Z","iopub.execute_input":"2024-01-09T06:47:41.194906Z","iopub.status.idle":"2024-01-09T06:47:41.985984Z","shell.execute_reply.started":"2024-01-09T06:47:41.194871Z","shell.execute_reply":"2024-01-09T06:47:41.984851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"psudo_df = None\n\nif CONFIG[\"psudo_train_size\"] != 0:\n    psudo_df = pd.read_csv(PSUDO_CSV_PATH)\n    psudo_df[\"file_path\"] = psudo_df[\"id\"].apply(lambda image_id: os.path.join(PSUDO_FOLDER, f\"{image_id}.tif\"))\n    psudo_df = psudo_df[(psudo_df[\"label\"]>=PSUDO_THR) | (psudo_df[\"label\"]<=(1-PSUDO_THR))] \n    psudo_df[\"label\"] = psudo_df[\"label\"].apply(lambda label: 1 if label>0.5 else 0)\n    print(\"total psudo len:\", len(psudo_df))\n    \n    if CONFIG[\"psudo_train_size\"] is not None:\n        psudo_df = psudo_df.sample(n=CONFIG[\"psudo_train_size\"])\n    ","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:47:41.987378Z","iopub.execute_input":"2024-01-09T06:47:41.987743Z","iopub.status.idle":"2024-01-09T06:47:41.996252Z","shell.execute_reply.started":"2024-01-09T06:47:41.987712Z","shell.execute_reply":"2024-01-09T06:47:41.995077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"group_fold = GroupKFold(n_splits=CONFIG[\"k_fold\"])\n\nfolds_id_train = []\nfolds_id_val = []\nfold_idx = 0\nfor train_index, test_index in group_fold.split(df['id'].values, df['label'].values, df['wsi'].values):\n    print(\"fold\", fold_idx, \"total train:\", len(train_index))\n    folds_id_train.append(df['id'].values[train_index])\n    folds_id_val.append(df['id'].values[test_index])\n    fold_idx += 1","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:47:41.997809Z","iopub.execute_input":"2024-01-09T06:47:41.998256Z","iopub.status.idle":"2024-01-09T06:47:42.360075Z","shell.execute_reply.started":"2024-01-09T06:47:41.998212Z","shell.execute_reply":"2024-01-09T06:47:42.358905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run training","metadata":{}},{"cell_type":"code","source":"models = []\nhistories = []\nbest_epoch_aurocs = []\n\nfor fold_idx in range(len(folds_id_train)):\n    \n    val_df = df[df[\"id\"].isin(folds_id_val[fold_idx])].sample(n=CONFIG[\"val_size\"])\n    # Make sure validation images are not in train data set\n    train_df = df[df[\"id\"].isin(folds_id_train[fold_idx])].sample(n=CONFIG[\"train_size\"])\n\n    if psudo_df is not None:\n        # no need to shuflle, shuffled in dataloader\n        train_df = pd.concat([train_df, psudo_df])\n    \n    train_dataloader, val_dataloader = prepare_loaders(train_df, val_df, tta_size=CONFIG[\"tta_size\"])\n    \n    model = HCDModel_Res50(num_classes=CONFIG[\"num_classes\"], weights=CONFIG[\"weights_name\"], dropout_p=CONFIG[\"dropout_p\"])\n    model.to(CONFIG['device'])\n    \n    \n    scheduler, optimizer = fetch_scheduler_optimizer(get_optim_params(model))\n\n    model, history, best_epoch_loss, best_epoch_auroc = run_training(\n        model, optimizer, scheduler,\n        device=CONFIG['device'],\n        num_epochs=CONFIG['epochs'],\n        stop_epochs=CONFIG[\"stop_epochs\"],\n        train_dataloader=train_dataloader,\n        val_dataloader=val_dataloader,\n        k_fold=fold_idx,\n        val_iterations=CONFIG[\"val_iterations\"],\n    )\n    \n    models.append(model)\n    histories.append(history)\n    best_epoch_aurocs.append(best_epoch_auroc)\n\n\nprint(\"Average best AUROCS:\", sum(best_epoch_aurocs)/len(best_epoch_aurocs))","metadata":{"execution":{"iopub.status.busy":"2024-01-09T06:49:03.731028Z","iopub.execute_input":"2024-01-09T06:49:03.731627Z","iopub.status.idle":"2024-01-09T06:49:25.743296Z","shell.execute_reply.started":"2024-01-09T06:49:03.731579Z","shell.execute_reply":"2024-01-09T06:49:25.741057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot results","metadata":{}},{"cell_type":"code","source":"for k_folder, history in enumerate(histories):\n    print(\"History of folder\", k_folder)\n    history = pd.DataFrame.from_dict(history)\n    history.to_csv(\"history.csv\", index=False)\n    \n    plot_history(history, \"Loss\")\n    plot_history(history, \"AUROC\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}