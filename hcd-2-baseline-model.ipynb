{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11848,"databundleVersionId":862157,"sourceType":"competition"},{"sourceId":157575662,"sourceType":"kernelVersion"}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Summary\n\nIn general, I plan to use transfer learning to load different pre-trained models as basis, and then the own model. \n\nAs a baseline model, I would start with a small and quick one \"tf_efficientnet_b0\" which would train on the original images\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Import libs","metadata":{}},{"cell_type":"code","source":"import os\nimport math\n\n# For data manipulation\nimport pandas as pd\n\n# Pytorch Imports\nimport torch\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\n# Utils\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:20:53.239468Z","iopub.execute_input":"2024-01-03T14:20:53.240166Z","iopub.status.idle":"2024-01-03T14:20:57.171707Z","shell.execute_reply.started":"2024-01-03T14:20:53.240128Z","shell.execute_reply":"2024-01-03T14:20:57.170785Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from hcd_2_common import (\n    fetch_scheduler,\n    generate_submit_csv,\n    load_weights,\n    prepare_loaders,\n    plot_history,\n    run_training,\n    set_seed,\n    HCDDataset,\n    data_transforms,\n    HCDModel,\n    CONFIG,\n    IS_INTERACTIVE,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:20:57.174341Z","iopub.execute_input":"2024-01-03T14:20:57.175025Z","iopub.status.idle":"2024-01-03T14:21:04.351440Z","shell.execute_reply.started":"2024-01-03T14:20:57.174980Z","shell.execute_reply":"2024-01-03T14:21:04.350272Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Configurations","metadata":{}},{"cell_type":"code","source":"ROOT_FOLDER = \"/kaggle/input/histopathologic-cancer-detection\"\nPREVIOUS_BEST_WEIGHT = \"/kaggle/input/hcd-2-baseline-model-original-images/AUROC0.94_Loss0.1720_epoch13.bin\"","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:21:04.352646Z","iopub.execute_input":"2024-01-03T14:21:04.353152Z","iopub.status.idle":"2024-01-03T14:21:04.357625Z","shell.execute_reply.started":"2024-01-03T14:21:04.353122Z","shell.execute_reply":"2024-01-03T14:21:04.356657Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Following configurations are often overwritten\n\nCONFIG[\"epochs\"] = 8\nCONFIG[\"train_size\"] = 8000\nCONFIG[\"val_size\"] = 800\nCONFIG[\"generate_submit\"] = True\nCONFIG[\"force_retrain\"] = True\n\nprint(\"CONFIG:\", CONFIG)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:21:04.360657Z","iopub.execute_input":"2024-01-03T14:21:04.361081Z","iopub.status.idle":"2024-01-03T14:21:04.391024Z","shell.execute_reply.started":"2024-01-03T14:21:04.361041Z","shell.execute_reply":"2024-01-03T14:21:04.389807Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"CONFIG: {'seed': 42, 'epochs': 8, 'img_size': 96, 'model_name': 'tf_efficientnet_b0', 'num_classes': 2, 'train_batch_size': 32, 'valid_batch_size': 32, 'learning_rate': 0.0001, 'scheduler': 'CosineAnnealingLR', 'min_lr': 1e-06, 'T_max': 500, 'weight_decay': 1e-06, 'train_size': 8000, 'val_size': 200, 'n_accumulate': 1, 'device': device(type='cpu'), 'generate_submit': True, 'force_retrain': True}\n","output_type":"stream"}]},{"cell_type":"code","source":"set_seed(CONFIG['seed'])","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:21:04.392401Z","iopub.execute_input":"2024-01-03T14:21:04.392892Z","iopub.status.idle":"2024-01-03T14:21:04.410329Z","shell.execute_reply.started":"2024-01-03T14:21:04.392846Z","shell.execute_reply":"2024-01-03T14:21:04.409402Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Load data","metadata":{}},{"cell_type":"code","source":"\ndf = pd.read_csv(os.path.join(ROOT_FOLDER, \"train_labels.csv\"))\ndf[\"file_path\"] = df[\"id\"].apply(lambda image_id: os.path.join(ROOT_FOLDER, \"train\", f\"{image_id}.tif\"))","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:21:04.412317Z","iopub.execute_input":"2024-01-03T14:21:04.413242Z","iopub.status.idle":"2024-01-03T14:21:05.548238Z","shell.execute_reply.started":"2024-01-03T14:21:04.413198Z","shell.execute_reply":"2024-01-03T14:21:05.547156Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"val_df = df.sample(n=CONFIG[\"val_size\"])\nval_ids = val_df[\"id\"].values\n# Make sure validation images are not in train data set\ntrain_df = df[~df[\"id\"].isin(val_ids)].sample(n=CONFIG[\"train_size\"])\n","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:21:05.549444Z","iopub.execute_input":"2024-01-03T14:21:05.549774Z","iopub.status.idle":"2024-01-03T14:21:05.626238Z","shell.execute_reply.started":"2024-01-03T14:21:05.549746Z","shell.execute_reply":"2024-01-03T14:21:05.625184Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train_loader, valid_loader = prepare_loaders(train_df, val_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:21:06.887781Z","iopub.execute_input":"2024-01-03T14:21:06.889174Z","iopub.status.idle":"2024-01-03T14:21:06.894909Z","shell.execute_reply.started":"2024-01-03T14:21:06.889127Z","shell.execute_reply":"2024-01-03T14:21:06.893971Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Run training","metadata":{}},{"cell_type":"code","source":"# Calculate the T_max, the max iterations\nCONFIG['T_max'] = train_df.shape[0] * CONFIG['epochs'] // CONFIG['train_batch_size']\nCONFIG['T_max']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = HCDModel(CONFIG['model_name'], CONFIG['num_classes'])\n\nweights_loaded = load_weights(model, PREVIOUS_BEST_WEIGHT)\n\nmodel.to(CONFIG['device']);","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = None\n\nif CONFIG['force_retrain'] or not weights_loaded:\n    optimizer = optim.Adam(model.parameters(), lr=CONFIG['learning_rate'], \n                           weight_decay=CONFIG['weight_decay'])\n    scheduler = fetch_scheduler(optimizer)\n\n    model, history = run_training(\n        model, optimizer, scheduler,\n        device=CONFIG['device'],\n        num_epochs=CONFIG['epochs'],\n        train_loader=train_loader,\n        valid_loader=valid_loader,\n    )\nelse:\n    print(\"Skip retrain model!\")","metadata":{"execution":{"iopub.status.busy":"2024-01-03T14:21:06.896559Z","iopub.execute_input":"2024-01-03T14:21:06.897128Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":" 48%|████▊     | 119/250 [01:51<01:58,  1.10it/s, Epoch=1, Epoch_Time=111, LR=9.91e-5, Train_AUROC=0.928, Train_Acc=0.933, Train_Loss=0.177]","output_type":"stream"}]},{"cell_type":"markdown","source":"## Plot results","metadata":{}},{"cell_type":"code","source":"if history is not None:\n    history = pd.DataFrame.from_dict(history)\n    history.to_csv(\"history.csv\", index=False)\n    \n    plot_history(history, \"Loss\")\n    plot_history(history, \"AUROC\")\n    plot_history(history, \"Accuracy\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"if CONFIG[\"generate_submit\"]:\n    generate_submit_csv(model, ROOT_FOLDER, data_transforms)\nelse:\n    print(\"Skip generating submit csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}